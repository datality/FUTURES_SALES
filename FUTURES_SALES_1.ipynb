{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": ".myenv",
   "display_name": ".myenv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# FUTURE SALES"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## INSTALLING PACKAGES"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U scikit-learn\n",
    "import sklearn as sklearn"
   ]
  },
  {
   "source": [
    "## LOAD DATA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER ='C:/Users/datal/OneDrive/Documents/Data Science Docs/COURSERA/KAGGLE/Final Project/'\n",
    "\n",
    "sales_train = pd.read_csv(os.path.join(DATA_FOLDER,'sales_train.csv'))\n",
    "\n",
    "\n",
    "items = pd.read_csv(os.path.join(DATA_FOLDER,'items.csv'))\n",
    "\n",
    "cats = pd.read_csv(os.path.join(DATA_FOLDER,'item_categories.csv'))\n",
    "\n",
    "shops = pd.read_csv(os.path.join(DATA_FOLDER,'shops.csv'))\n",
    "\n",
    "test = pd.read_csv(os.path.join(DATA_FOLDER,'test.csv'))"
   ]
  },
  {
   "source": [
    "## DATA CLEANING"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove outliers\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.xlim(-100, 3000)\n",
    "flierprops = dict(marker='o', markerfacecolor='purple', markersize=6,\n",
    "                  linestyle='none', markeredgecolor='black')\n",
    "sb.boxplot(x=sales_train.item_cnt_day, flierprops=flierprops)\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.xlim(sales_train.item_price.min(), sales_train.item_price.max()*1.1)\n",
    "sb.boxplot(x=sales_train.item_price, flierprops=flierprops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REMOVE OUTLIERS 2\n",
    "\n",
    "train = sales_train[(sales_train.item_price < 300000 )& (sales_train.item_cnt_day < 1000)]\n",
    "\n",
    "train = train[train.item_price > 0].reset_index(drop = True)\n",
    "train.loc[train.item_cnt_day < 1, \"item_cnt_day\"] = 0"
   ]
  },
  {
   "source": [
    "### SHOP DATA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Якутск Орджоникидзе, 56\n",
    "train.loc[train.shop_id == 0, 'shop_id'] = 57\n",
    "test.loc[test.shop_id == 0, 'shop_id'] = 57\n",
    "# Якутск ТЦ \"Центральный\"\n",
    "train.loc[train.shop_id == 1, 'shop_id'] = 58\n",
    "test.loc[test.shop_id == 1, 'shop_id'] = 58\n",
    "# Жуковский ул. Чкалова 39м²\n",
    "train.loc[train.shop_id == 10, 'shop_id'] = 11\n",
    "test.loc[test.shop_id == 10, 'shop_id'] = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group Category\n",
    "shops.loc[ shops.shop_name == 'Сергиев Посад ТЦ \"7Я\"',\"shop_name\" ] = 'СергиевПосад ТЦ \"7Я\"'\n",
    "shops[\"city\"] = shops.shop_name.str.split(\" \").map( lambda x: x[0] )\n",
    "shops[\"category\"] = shops.shop_name.str.split(\" \").map( lambda x: x[1] )\n",
    "shops.loc[shops.city == \"!Якутск\", \"city\"] = \"Якутск\"\n",
    "\n",
    "\n",
    "category = []\n",
    "\n",
    "for cat in shops.category.unique():\n",
    "    if len(shops[shops.category == cat]) >= 5:\n",
    "        category.append(cat)\n",
    "shops.category = shops.category.apply( lambda x: x if (x in category) else \"other\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "shops[\"shop_category\"] = LabelEncoder().fit_transform( shops.category )\n",
    "shops[\"shop_city\"] = LabelEncoder().fit_transform( shops.city )\n",
    "shops = shops[[\"shop_id\", \"shop_category\", \"shop_city\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning Item Category Data\n",
    "\n",
    "cats[\"type_code\"] = cats.item_category_name.apply( lambda x: x.split(\" \")[0] ).astype(str)\n",
    "cats.loc[ (cats.type_code == \"Игровые\")| (cats.type_code == \"Аксессуары\"), \"category\" ] = \"Игры\"\n",
    "\n",
    "category = []\n",
    "for cat in cats.type_code.unique():\n",
    "    if len(cats[cats.type_code == cat]) >= 5: \n",
    "        category.append( cat )\n",
    "cats.type_code = cats.type_code.apply(lambda x: x if (x in category) else \"etc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats.type_code = LabelEncoder().fit_transform(cats.type_code)\n",
    "cats[\"split\"] = cats.item_category_name.apply(lambda x: x.split(\"-\"))\n",
    "cats[\"subtype\"] = cats.split.apply(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\n",
    "cats[\"subtype_code\"] = LabelEncoder().fit_transform( cats[\"subtype\"] )\n",
    "cats = cats[[\"item_category_id\", \"subtype_code\", \"type_code\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning Item Data\n",
    "\n",
    "import re\n",
    "def name_correction(x):\n",
    "    x = x.lower() # all letters lower case\n",
    "    x = x.partition('[')[0] # partition by square brackets\n",
    "    x = x.partition('(')[0] # partition by curly brackets\n",
    "    x = re.sub('[^A-Za-z0-9А-Яа-я]+', ' ', x) # remove special characters\n",
    "    x = x.replace('  ', ' ') # replace double spaces with single spaces\n",
    "    x = x.strip() # remove leading and trailing white space\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split item names by first bracket\n",
    "items[\"name1\"], items[\"name2\"] = items.item_name.str.split(\"[\", 1).str\n",
    "items[\"name1\"], items[\"name3\"] = items.item_name.str.split(\"(\", 1).str\n",
    "\n",
    "# replace special characters and turn to lower case\n",
    "items[\"name2\"] = items.name2.str.replace('[^A-Za-z0-9А-Яа-я]+', \" \").str.lower()\n",
    "items[\"name3\"] = items.name3.str.replace('[^A-Za-z0-9А-Яа-я]+', \" \").str.lower()\n",
    "\n",
    "# fill nulls with '0'\n",
    "items = items.fillna('0')\n",
    "\n",
    "items[\"item_name\"] = items[\"item_name\"].apply(lambda x: name_correction(x))\n",
    "\n",
    "# return all characters except the last if name 2 is not \"0\" - the closing bracket\n",
    "items.name2 = items.name2.apply( lambda x: x[:-1] if x !=\"0\" else \"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean item type\n",
    "\n",
    "items[\"type\"] = items.name2.apply(lambda x: x[0:8] if x.split(\" \")[0] == \"xbox\" else x.split(\" \")[0] )\n",
    "items.loc[(items.type == \"x360\") | (items.type == \"xbox360\") | (items.type == \"xbox 360\") ,\"type\"] = \"xbox 360\"\n",
    "items.loc[ items.type == \"\", \"type\"] = \"mac\"\n",
    "items.type = items.type.apply( lambda x: x.replace(\" \", \"\") )\n",
    "items.loc[ (items.type == 'pc' )| (items.type == 'pс') | (items.type == \"pc\"), \"type\" ] = \"pc\"\n",
    "items.loc[ items.type == 'рs3' , \"type\"] = \"ps3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_sum = items.groupby([\"type\"]).agg({\"item_id\": \"count\"})\n",
    "group_sum = group_sum.reset_index()\n",
    "drop_cols = []\n",
    "for cat in group_sum.type.unique():\n",
    "    if group_sum.loc[(group_sum.type == cat), \"item_id\"].values[0] <40:\n",
    "        drop_cols.append(cat)\n",
    "items.name2 = items.name2.apply( lambda x: \"other\" if (x in drop_cols) else x )\n",
    "items = items.drop([\"type\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items.name2 = LabelEncoder().fit_transform(items.name2)\n",
    "items.name3 = LabelEncoder().fit_transform(items.name3)\n",
    "\n",
    "items.drop([\"item_name\", \"name1\"],axis = 1, inplace= True)\n",
    "items.head()"
   ]
  },
  {
   "source": [
    "## PREPROCESSING\n",
    "\n",
    "Create a matrix df with every combination of month, shop and item in order of increasing month. Item_cnt_day is summed into an item_cnt_month."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import time\n",
    "ts = time.time()\n",
    "matrix = []\n",
    "cols  = [\"date_block_num\", \"shop_id\", \"item_id\"]\n",
    "for i in range(34):\n",
    "    sales = train[train.date_block_num == i]\n",
    "    matrix.append( np.array(list( product( [i], sales.shop_id.unique(), sales.item_id.unique() ) ), dtype = np.int16) )\n",
    "\n",
    "matrix = pd.DataFrame( np.vstack(matrix), columns = cols )\n",
    "matrix[\"date_block_num\"] = matrix[\"date_block_num\"].astype(np.int8)\n",
    "matrix[\"shop_id\"] = matrix[\"shop_id\"].astype(np.int8)\n",
    "matrix[\"item_id\"] = matrix[\"item_id\"].astype(np.int16)\n",
    "matrix.sort_values( cols, inplace = True )\n",
    "time.time()- ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add revenue to train df\n",
    "train[\"revenue\"] = train[\"item_cnt_day\"] * train[\"item_price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "group = train.groupby( [\"date_block_num\", \"shop_id\", \"item_id\"] ).agg( {\"item_cnt_day\": [\"sum\"]} )\n",
    "group.columns = [\"item_cnt_month\"]\n",
    "group.reset_index( inplace = True)\n",
    "matrix = pd.merge( matrix, group, on = cols, how = \"left\" )\n",
    "matrix[\"item_cnt_month\"] = matrix[\"item_cnt_month\"].fillna(0).astype(np.float16)\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"date_block_num\"] = 34\n",
    "test[\"date_block_num\"] = test[\"date_block_num\"].astype(np.int8)\n",
    "test[\"shop_id\"] = test.shop_id.astype(np.int8)\n",
    "test[\"item_id\"] = test.item_id.astype(np.int16)"
   ]
  },
  {
   "source": [
    "#Concatenate train and test sets.\n",
    "\n",
    "ts = time.time()\n",
    "\n",
    "matrix = pd.concat([matrix, test.drop([\"ID\"],axis = 1)], ignore_index=True, sort=False, keys=cols)\n",
    "matrix.fillna( 0, inplace = True )\n",
    "time.time() - ts\n",
    "\n",
    "\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add shop, items and categories data onto matrix df.\n",
    "\n",
    "ts = time.time()\n",
    "matrix = pd.merge( matrix, shops, on = [\"shop_id\"], how = \"left\" )\n",
    "matrix = pd.merge(matrix, items, on = [\"item_id\"], how = \"left\")\n",
    "matrix = pd.merge( matrix, cats, on = [\"item_category_id\"], how = \"left\" )\n",
    "matrix[\"shop_city\"] = matrix[\"shop_city\"].astype(np.int8)\n",
    "matrix[\"shop_category\"] = matrix[\"shop_category\"].astype(np.int8)\n",
    "matrix[\"item_category_id\"] = matrix[\"item_category_id\"].astype(np.int8)\n",
    "matrix[\"subtype_code\"] = matrix[\"subtype_code\"].astype(np.int8)\n",
    "matrix[\"name2\"] = matrix[\"name2\"].astype(np.int8)\n",
    "matrix[\"name3\"] = matrix[\"name3\"].astype(np.int16)\n",
    "matrix[\"type_code\"] = matrix[\"type_code\"].astype(np.int8)\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Description of training set\")\n",
    "print(\"Shape: \\t\\t\\t\", matrix.shape)\n",
    "print(\"#NaNs: \\t\\t\\t\", matrix.isna().sum().sum()) # No NaN\n",
    "print(\"#Shops: \\t\\t\", matrix.shop_id.nunique())\n",
    "print(\"#Item Categories: \\t\", matrix.item_category_id.nunique())\n",
    "print(\"#Items: \\t\\t\", matrix.item_id.nunique())\n",
    "print(\"#Months: \\t\\t\", matrix.date_block_num.nunique())\n",
    "print(\"Units Sold range from: \\t\", matrix.item_cnt_month.min(), \" to \", matrix.item_cnt_month.max())"
   ]
  },
  {
   "source": [
    "## Feature Engineering\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LAG FEATURES\n",
    "\n",
    "# Define a lag feature function\n",
    "def lag_feature( df,lags, cols ):\n",
    "    for col in cols:\n",
    "        print(col)\n",
    "        tmp = df[[\"date_block_num\", \"shop_id\",\"item_id\",col ]]\n",
    "        for i in lags:\n",
    "            shifted = tmp.copy()\n",
    "            shifted.columns = [\"date_block_num\", \"shop_id\", \"item_id\", col + \"_lag_\"+str(i)]\n",
    "            shifted.date_block_num = shifted.date_block_num + i\n",
    "            df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add item_cnt_month lag features.\n",
    "\n",
    "\n",
    "\n",
    "ts = time.time()\n",
    "matrix = lag_feature( matrix, [1,2,3], [\"item_cnt_month\"] )\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the previous month's average item_cnt.\n",
    "\n",
    "ts = time.time()\n",
    "group = matrix.groupby( [\"date_block_num\"] ).agg({\"item_cnt_month\" : [\"mean\"]})\n",
    "group.columns = [\"date_avg_item_cnt\"]\n",
    "group.reset_index(inplace = True)\n",
    "\n",
    "matrix = pd.merge(matrix, group, on = [\"date_block_num\"], how = \"left\")\n",
    "matrix.date_avg_item_cnt = matrix[\"date_avg_item_cnt\"].astype(np.float16)\n",
    "matrix = lag_feature( matrix, [1], [\"date_avg_item_cnt\"] )\n",
    "matrix.drop( [\"date_avg_item_cnt\"], axis = 1, inplace = True )\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add lag values of item_cnt_month for month / item_id.\n",
    "\n",
    "ts = time.time()\n",
    "group = matrix.groupby(['date_block_num', 'item_id']).agg({'item_cnt_month': ['mean']})\n",
    "group.columns = [ 'date_item_avg_item_cnt' ]\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "matrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\n",
    "matrix.date_item_avg_item_cnt = matrix['date_item_avg_item_cnt'].astype(np.float16)\n",
    "matrix = lag_feature(matrix, [1,2,3], ['date_item_avg_item_cnt'])\n",
    "matrix.drop(['date_item_avg_item_cnt'], axis=1, inplace=True)\n",
    "time.time() - ts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add lag values for item_cnt_month for every month / shop combination.\n",
    "\n",
    "ts = time.time()\n",
    "group = matrix.groupby( [\"date_block_num\",\"shop_id\"] ).agg({\"item_cnt_month\" : [\"mean\"]})\n",
    "group.columns = [\"date_shop_avg_item_cnt\"]\n",
    "group.reset_index(inplace = True)\n",
    "\n",
    "matrix = pd.merge(matrix, group, on = [\"date_block_num\",\"shop_id\"], how = \"left\")\n",
    "matrix.date_avg_item_cnt = matrix[\"date_shop_avg_item_cnt\"].astype(np.float16)\n",
    "matrix = lag_feature( matrix, [1,2,3], [\"date_shop_avg_item_cnt\"] )\n",
    "matrix.drop( [\"date_shop_avg_item_cnt\"], axis = 1, inplace = True )\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add lag values for item_cnt_month for month/shop/item.\n",
    "\n",
    "\n",
    "ts = time.time()\n",
    "group = matrix.groupby( [\"date_block_num\",\"shop_id\",\"item_id\"] ).agg({\"item_cnt_month\" : [\"mean\"]})\n",
    "group.columns = [\"date_shop_item_avg_item_cnt\"]\n",
    "group.reset_index(inplace = True)\n",
    "\n",
    "matrix = pd.merge(matrix, group, on = [\"date_block_num\",\"shop_id\",\"item_id\"], how = \"left\")\n",
    "matrix.date_avg_item_cnt = matrix[\"date_shop_item_avg_item_cnt\"].astype(np.float16)\n",
    "matrix = lag_feature( matrix, [1,2,3], [\"date_shop_item_avg_item_cnt\"] )\n",
    "matrix.drop( [\"date_shop_item_avg_item_cnt\"], axis = 1, inplace = True )\n",
    "time.time() - ts\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add lag values for item_cnt_month for month/shop/item subtype.\n",
    "\n",
    "ts = time.time()\n",
    "group = matrix.groupby(['date_block_num', 'shop_id', 'subtype_code']).agg({'item_cnt_month': ['mean']})\n",
    "group.columns = ['date_shop_subtype_avg_item_cnt']\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "matrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'subtype_code'], how='left')\n",
    "matrix.date_shop_subtype_avg_item_cnt = matrix['date_shop_subtype_avg_item_cnt'].astype(np.float16)\n",
    "matrix = lag_feature(matrix, [1], ['date_shop_subtype_avg_item_cnt'])\n",
    "matrix.drop(['date_shop_subtype_avg_item_cnt'], axis=1, inplace=True)\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add lag values for item_cnt_month for month/city.\n",
    "\n",
    "\n",
    "s = time.time()\n",
    "group = matrix.groupby(['date_block_num', 'shop_city']).agg({'item_cnt_month': ['mean']})\n",
    "group.columns = ['date_city_avg_item_cnt']\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "matrix = pd.merge(matrix, group, on=['date_block_num', \"shop_city\"], how='left')\n",
    "matrix.date_city_avg_item_cnt = matrix['date_city_avg_item_cnt'].astype(np.float16)\n",
    "matrix = lag_feature(matrix, [1], ['date_city_avg_item_cnt'])\n",
    "matrix.drop(['date_city_avg_item_cnt'], axis=1, inplace=True)\n",
    "time.time() - ts\n",
    "\n",
    "#Add lag values for item_cnt_month for month/city/item.\n",
    "\n",
    "ts = time.time()\n",
    "group = matrix.groupby(['date_block_num', 'item_id', 'shop_city']).agg({'item_cnt_month': ['mean']})\n",
    "group.columns = [ 'date_item_city_avg_item_cnt' ]\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "matrix = pd.merge(matrix, group, on=['date_block_num', 'item_id', 'shop_city'], how='left')\n",
    "matrix.date_item_city_avg_item_cnt = matrix['date_item_city_avg_item_cnt'].astype(np.float16)\n",
    "matrix = lag_feature(matrix, [1], ['date_item_city_avg_item_cnt'])\n",
    "matrix.drop(['date_item_city_avg_item_cnt'], axis=1, inplace=True)\n",
    "\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add average item price on to matix df.\n",
    "#Add lag values of item price per month.\n",
    "#Add delta price values - how current month average pirce relates to global average.\n",
    "\n",
    "\n",
    "ts = time.time()\n",
    "group = train.groupby( [\"item_id\"] ).agg({\"item_price\": [\"mean\"]})\n",
    "group.columns = [\"item_avg_item_price\"]\n",
    "group.reset_index(inplace = True)\n",
    "\n",
    "matrix = matrix.merge( group, on = [\"item_id\"], how = \"left\" )\n",
    "matrix[\"item_avg_item_price\"] = matrix.item_avg_item_price.astype(np.float16)\n",
    "\n",
    "\n",
    "group = train.groupby( [\"date_block_num\",\"item_id\"] ).agg( {\"item_price\": [\"mean\"]} )\n",
    "group.columns = [\"date_item_avg_item_price\"]\n",
    "group.reset_index(inplace = True)\n",
    "\n",
    "matrix = matrix.merge(group, on = [\"date_block_num\",\"item_id\"], how = \"left\")\n",
    "matrix[\"date_item_avg_item_price\"] = matrix.date_item_avg_item_price.astype(np.float16)\n",
    "lags = [1, 2, 3]\n",
    "matrix = lag_feature( matrix, lags, [\"date_item_avg_item_price\"] )\n",
    "for i in lags:\n",
    "    matrix[\"delta_price_lag_\" + str(i) ] = (matrix[\"date_item_avg_item_price_lag_\" + str(i)]- matrix[\"item_avg_item_price\"] )/ matrix[\"item_avg_item_price\"]\n",
    "\n",
    "def select_trends(row) :\n",
    "    for i in lags:\n",
    "        if row[\"delta_price_lag_\" + str(i)]:\n",
    "            return row[\"delta_price_lag_\" + str(i)]\n",
    "    return 0\n",
    "\n",
    "matrix[\"delta_price_lag\"] = matrix.apply(select_trends, axis = 1)\n",
    "matrix[\"delta_price_lag\"] = matrix.delta_price_lag.astype( np.float16 )\n",
    "matrix[\"delta_price_lag\"].fillna( 0 ,inplace = True)\n",
    "\n",
    "features_to_drop = [\"item_avg_item_price\", \"date_item_avg_item_price\"]\n",
    "for i in lags:\n",
    "    features_to_drop.append(\"date_item_avg_item_price_lag_\" + str(i) )\n",
    "    features_to_drop.append(\"delta_price_lag_\" + str(i) )\n",
    "matrix.drop(features_to_drop, axis = 1, inplace = True)\n",
    "time.time() - ts"
   ]
  },
  {
   "source": [
    "Add total shop revenue per month to matix df.\n",
    "Add lag values of revenue per month.\n",
    "Add delta revenue values - how current month revenue relates to global average."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "group = train.groupby( [\"date_block_num\",\"shop_id\"] ).agg({\"revenue\": [\"sum\"] })\n",
    "group.columns = [\"date_shop_revenue\"]\n",
    "group.reset_index(inplace = True)\n",
    "\n",
    "matrix = matrix.merge( group , on = [\"date_block_num\", \"shop_id\"], how = \"left\" )\n",
    "matrix['date_shop_revenue'] = matrix['date_shop_revenue'].astype(np.float32)\n",
    "\n",
    "group = group.groupby([\"shop_id\"]).agg({ \"date_block_num\":[\"mean\"] })\n",
    "group.columns = [\"shop_avg_revenue\"]\n",
    "group.reset_index(inplace = True )\n",
    "\n",
    "matrix = matrix.merge( group, on = [\"shop_id\"], how = \"left\" )\n",
    "matrix[\"shop_avg_revenue\"] = matrix.shop_avg_revenue.astype(np.float32)\n",
    "matrix[\"delta_revenue\"] = (matrix['date_shop_revenue'] - matrix['shop_avg_revenue']) / matrix['shop_avg_revenue']\n",
    "matrix[\"delta_revenue\"] = matrix[\"delta_revenue\"]. astype(np.float32)\n",
    "\n",
    "matrix = lag_feature(matrix, [1], [\"delta_revenue\"])\n",
    "matrix[\"delta_revenue_lag_1\"] = matrix[\"delta_revenue_lag_1\"].astype(np.float32)\n",
    "matrix.drop( [\"date_shop_revenue\", \"shop_avg_revenue\", \"delta_revenue\"] ,axis = 1, inplace = True)\n",
    "time.time() - ts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add month and number of days in each month to matrix df.\n",
    "\n",
    "matrix[\"month\"] = matrix[\"date_block_num\"] % 12\n",
    "days = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\n",
    "matrix[\"days\"] = matrix[\"month\"].map(days).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the month of each shop and item first sale.\n",
    "\n",
    "matrix[\"item_shop_first_sale\"] = matrix[\"date_block_num\"] - matrix.groupby([\"item_id\",\"shop_id\"])[\"date_block_num\"].transform('min')\n",
    "matrix[\"item_first_sale\"] = matrix[\"date_block_num\"] - matrix.groupby([\"item_id\"])[\"date_block_num\"].transform('min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete first three months from matrix. They don't have lag values.\n",
    "matrix = matrix[matrix[\"date_block_num\"] > 3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix.info()\n"
   ]
  },
  {
   "source": [
    "### Plotting Overall Monthly trend¶\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train[['date_block_num', 'item_id']].groupby(by='date_block_num', as_index=False).nunique()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,4))\n",
    "plt.plot(df.item_id, sb.xkcd_rgb[\"denim blue\"], lw=4)\n",
    "plt.title(\"Count of Unique items sold\")\n",
    "\n",
    "\n",
    "df = train[['date_block_num', 'item_cnt_day']].groupby(by='date_block_num', as_index=False).sum()\n",
    "\n",
    "f, ax = plt.subplots(figsize=(16, 4))\n",
    "plt.plot(df.item_cnt_day, sb.xkcd_rgb[\"denim blue\"], lw=4)\n",
    "plt.title(\"Sum of items sold\")\n",
    "\n",
    "\n",
    "df = train[['date_block_num', 'revenue']].groupby(by='date_block_num', as_index=False).sum()\n",
    "\n",
    "f, ax = plt.subplots(figsize=(16, 4))\n",
    "plt.plot(df.revenue, sb.xkcd_rgb[\"denim blue\"], lw=4)\n",
    "plt.title(\"Total revenue generated\");"
   ]
  },
  {
   "source": [
    "Number of Unique items and Total Units Sold are decreasig with time. Revenue trend looks relatively flat"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Plotting same metrics for each shop"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_hm = train.pivot_table(index='date_block_num', columns='shop_id', values='item_id', aggfunc='nunique', fill_value=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,4))\n",
    "cmap = sb.cubehelix_palette(8, as_cmap=True, dark=0, light=1, gamma=0.8, reverse=True)\n",
    "plt.title(\"Count of Unique items sold\")\n",
    "_ = sb.heatmap(stores_hm, ax=ax, cmap=cmap, cbar=False, xticklabels=False)\n",
    "\n",
    "\n",
    "stores_hm = train.pivot_table(index='date_block_num', columns='shop_id', values='item_cnt_day', aggfunc='sum', fill_value=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,4))\n",
    "cmap = sb.cubehelix_palette(8, as_cmap=True, dark=0, light=1, gamma = .8, reverse=True)\n",
    "plt.title(\"Sum of items sold\")\n",
    "_ = sb.heatmap(stores_hm, ax=ax, cmap=cmap, cbar=False, xticklabels=False)\n",
    "\n",
    "\n",
    "stores_hm = train.pivot_table(index='date_block_num', columns='shop_id', values='revenue', aggfunc='sum', fill_value=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,4))\n",
    "cmap = sb.cubehelix_palette(8, as_cmap=True, dark=0, light=1, gamma = .8, reverse=True)\n",
    "plt.title(\"Total revenue generated\")\n",
    "_ = sb.heatmap(stores_hm, ax=ax, cmap=cmap, cbar=False, xticklabels=False)"
   ]
  },
  {
   "source": [
    "Black areas indicate 0 Not all 60 shops are in operations; few shops with light color are the main drivers"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Plotting same metrics for each item category"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_hm = matrix.pivot_table(index='date_block_num', columns='item_category_id', values='item_id', aggfunc='nunique', fill_value=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,4))\n",
    "cmap = sb.cubehelix_palette(8, as_cmap=True, dark=0, light=1, gamma=0.8, reverse=True)\n",
    "plt.title(\"Count of Unique items sold\")\n",
    "_ = sb.heatmap(stores_hm, ax=ax, cmap=cmap, cbar=False, xticklabels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix.head().T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install xgboost\n",
    "!pip install lightgbm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "import calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save dataset\n",
    "\n",
    "matrix.to_pickle('matrix.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('matrix.pkl')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df[df.date_block_num < 33].drop(['item_cnt_month'], axis=1)\n",
    "Y_train = df[df.date_block_num < 33]['item_cnt_month']\n",
    "X_valid = df[df.date_block_num == 33].drop(['item_cnt_month'], axis=1)\n",
    "Y_valid = df[df.date_block_num == 33]['item_cnt_month']\n",
    "X_test = df[df.date_block_num == 34].drop(['item_cnt_month'], axis=1)\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = Y_train.clip(0, 20)\n",
    "Y_valid = Y_valid.clip(0, 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "\n",
    "model = XGBRegressor(\n",
    "    max_depth=10,\n",
    "    n_estimators=1000,\n",
    "    min_child_weight=0.5, \n",
    "    colsample_bytree=0.8, \n",
    "    subsample=0.8, \n",
    "    eta=0.1,\n",
    "#     tree_method='gpu_hist',\n",
    "    seed=42)\n",
    "\n",
    "model.fit(\n",
    "    X_train, \n",
    "    Y_train, \n",
    "    eval_metric=\"rmse\", \n",
    "    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n",
    "    verbose=True, \n",
    "    early_stopping_rounds = 20)\n",
    "\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_valid).clip(0, 20)\n",
    "Y_test = model.predict(X_test).clip(0, 20)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"ID\": test.index, \n",
    "    \"item_cnt_month\": Y_test\n",
    "})\n",
    "submission.to_csv('xgboost_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance\n",
    "\n",
    "def plot_features(booster, figsize):    \n",
    "    fig, ax = plt.subplots(1,1,figsize=figsize)\n",
    "    return plot_importance(booster=booster, ax=ax)\n",
    "\n",
    "plot_features(model, (10,14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}